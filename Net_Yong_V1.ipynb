{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55458c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator \n",
    "# - using the synthetic images to train our network\n",
    "import os\n",
    "import numpy as np\n",
    "from config import config\n",
    "from fringes import fringe_wrapper\n",
    "from fringes import Fringes_Generator \n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter   \n",
    "import torch,gc\n",
    "\n",
    "cfg = config()\n",
    "# class Fringes_Generator():\n",
    "#     \"\"\"\n",
    "#     A distributed version of fringes.py. That says, the A is NOT a constant value across the whole image.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config):\n",
    "#         self._c = config\n",
    "#         self.SIG = fringe_wrapper(self._c, \"harmonic\") # synthetic image generator\n",
    "\n",
    "#     def update_cfg(self):\n",
    "#         A0 = np.random.choice(np.linspace(50,150,101)) + np.random.randn()#得到50-150之间一个属\n",
    "#         self._c.A = [np.reshape(np.linspace(A0, 150, self._c.pattern_size[1]), (-1,1))]*3 #生成数组\n",
    "#         A_min, A_max = np.min(self._c.A[0]), np.max(self._c.A[0])\n",
    "#         B_max = np.min([A_min, 255-A_max])\n",
    "#         self._c.B = [(B_max-10)*np.random.rand()+10]*3\n",
    "#         self._c.C = 20*np.random.rand()-10 # -10,10 np.random.rand()返回的是标准正态分布的一个值\n",
    "#         self._c.D = 10*np.random.rand()-5  # -5,5\n",
    "#         self._c.E = 6*np.random.rand()-3   # -3,3\n",
    "#         self._c.F = 4*np.random.rand()-2   # -2,2\n",
    "#         self._c.gamma = 0.8*np.random.rand()+1.5 # 0.7,2.3\n",
    "#         self._c.parameter_array = [A0,self._c.B[0],self._c.C,self._c.D,self._c.E,self._c.F]\n",
    "\n",
    "# #         method = np.random.choice([\"fringe\", \"gamma\", \"harmonic\"])\n",
    "# #         self.SIG = fringe_wrapper(self._c, method) # update the synthetic image generator \n",
    "# #         print(self._c.B[0], self._c.C, self._c.D, self._c.E, self._c.F, self._c.gamma)\n",
    "\n",
    "\n",
    "#     def save_data(self, ind):\n",
    "#         self.update_cfg() # 整个系统重新配置一遍。\n",
    "#         images = self.SIG.generate_all() # 生成图像\n",
    "#         if ind < 20:\n",
    "#             for f, fringes in enumerate(images):\n",
    "#                 for s, fringe in enumerate(fringes):\n",
    "#                     p = Path(self._c.pattern_path)/f\"{ind:0>3d}{f:0>2d}{s:0>2d}{self._c.hv}.bmp\" \n",
    "#                     cv2.imwrite(str(p), fringe)\n",
    "#         np.savez(self._c.net_dir+f\"train_data{ind:03d}\", image1=images[0], image2=images[1], image3=images[2]) #将3个条纹保存在未压缩的.npz格式中\n",
    "#         print(f\"{ind}th syn data is saved\")\n",
    "cfg.net_dir=\"./data/train_gamma/\"\n",
    "cfg.model_dir = \"\"\n",
    "cfg.pattern_size = [1920, 50] #[1920, 50] [512, 1]\n",
    "cfg.Tp = [30, 33, 36]       #在这个实验中，就先设置成这个周期。\n",
    "cfg.steps = [7,4,3]\n",
    "\n",
    "\n",
    "#删除文件\n",
    "def del_files(path_file):\n",
    "    ls = os.listdir(path_file)\n",
    "    for i in ls:\n",
    "        f_path = os.path.join(path_file, i)\n",
    "        # 判断是否是一个目录,若是,则递归删除\n",
    "        if os.path.isdir(f_path):\n",
    "            del_files(f_path)\n",
    "        else:\n",
    "            os.remove(f_path)\n",
    "\n",
    "a = input(\"是否需要重新生成数据集，请输入Y或者N ：\")\n",
    "if a == 'Y':\n",
    "    cfg.pattern_path = './data/fringe_gamma'\n",
    "\n",
    "    if not os.path.exists(cfg.pattern_path):\n",
    "        os.makedirs(cfg.pattern_path)\n",
    "    if not os.path.exists(cfg.net_dir):\n",
    "        os.makedirs(cfg.net_dir)\n",
    "\n",
    "    del_files(cfg.pattern_path+\"/\")\n",
    "    del_files(cfg.net_dir)\n",
    "    for i in range(100):\n",
    "        cfg.gamma = random.uniform(0.8,2.0)\n",
    "        print(cfg.gamma)\n",
    "        images = fringe_wrapper(cfg, \"gamma\").generate_all()\n",
    "        np.savez(f\"./data/train_gamma/train_data{i:03d}\", image1=images[0], image2=images[1], image3=images[2]) \n",
    "    # for ind in range(100):\n",
    "    #     fringe_generator=Fringes_Generator(cfg)\n",
    "    #     fringe_generator.save_data(ind)\n",
    "\n",
    "\n",
    "b = input(\"是否需要重新开始训练，请输入Y或者N ：\")\n",
    "if b == 'Y':\n",
    "    if not os.path.exists(\"./data/model_gamma\"):\n",
    "        os.makedirs(\"./data/model_gamma\")\n",
    "    del_files(\"./data/model_gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0530e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.C :  tensor([[ 0.8019,  0.3569,  0.3569,  0.8019],\n",
      "        [ 1.0000,  1.4450, -0.8019, -0.8019],\n",
      "        [-0.8019, -0.8019,  1.4450,  1.0000]], device='cuda:0')\n",
      "加载best_model成功！其为epoch19\n"
     ]
    }
   ],
   "source": [
    "# build a MLP model for residual estimation \n",
    "#- The linear part is also considered in this network\n",
    "from torch import nn\n",
    "\n",
    "# 定义训练的设备\n",
    "device = torch.device(\"cuda\") #使用gpu进行训练\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()#清楚cuda缓存\n",
    "\n",
    "class NeuralNetwork(nn.Module): #构建CNN神经网络\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.MLP1 = nn.Sequential(\n",
    "            nn.Conv2d(9,  64, 1, stride=1),  #四个卷积层\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 256, 1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(256, 128, 1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(128, 4, 1, stride=1),\n",
    "        )\n",
    "        \n",
    "        A = [[1, np.cos(0), np.sin(0)],                \n",
    "             [1, np.cos(6*np.pi/7), np.sin(6*np.pi/7)],\n",
    "             [1, np.cos(8*np.pi/7), np.sin(8*np.pi/7)]]\n",
    "        B = [[1, np.cos(2*np.pi/7), np.sin(2*np.pi/7)],\n",
    "             [1, np.cos(4*np.pi/7), np.sin(4*np.pi/7)],\n",
    "             [1, np.cos(10*np.pi/7), np.sin(10*np.pi/7)],\n",
    "             [1, np.cos(12*np.pi/7), np.sin(12*np.pi/7)]]\n",
    "        A, B = np.array(A), np.array(B) #列表转化成数组\n",
    "        self.C = torch.Tensor(np.matmul(B, np.linalg.inv(A)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.C = self.C.to(device)\n",
    "        print(\"self.C : \",self.C)\n",
    "        D = [[1, np.cos(0), np.sin(0)],\n",
    "            [1, np.cos(2*np.pi/4), np.sin(2*np.pi/4)],\n",
    "            [1, np.cos(4*np.pi/4), np.sin(4*np.pi/4)]]\n",
    "        E = [[1, np.cos(6*np.pi/4), np.sin(6*np.pi/4)]]\n",
    "        self.F = torch.Tensor(np.matmul(E, np.linalg.pinv(D)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.F = self.F.to(device)\n",
    "        self.model_save_dir = \"./data/model_gamma\" #定义保存模型的文件夹\n",
    "        self.p = os.path.join(self.model_save_dir,\"best_model\")\n",
    "        self.last_loss = 9999999999\n",
    "        self.last_epoch = 0\n",
    "        \n",
    "        #对数组A求逆矩阵，与B矩阵相乘\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # linear generator for the 4 images\n",
    "        images = images.to(device)\n",
    "        i7s = images.permute(0, 2, 3,1)            #将tensor的维度换位\n",
    "        i7s,_ = torch.split(i7s,(3,6),dim=-1)      #切分，得到的就是I1 I7 I8\n",
    "\n",
    "        linearPart = torch.matmul(i7s, self.C+0.0) #Y矩阵乘法 50*(1950*3)矩阵乘以3*4矩阵\n",
    "        linearPart = linearPart.to(device)\n",
    "        linearPart = linearPart.permute(0,3,1,2)   #维度换位\n",
    "        \n",
    "        # The nonliear part is estimated with MLP function \n",
    "        images = images/255\n",
    "        images = images.to(device)\n",
    "        res = self.MLP1(images)\n",
    "\n",
    "        return linearPart + res\n",
    "    \n",
    "    def save(self,optimizer,epoch,loss,batch): #TODO\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        m = os.path.join(self.model_save_dir,\"model\"+str(epoch))\n",
    "\n",
    "        state = {'model':self.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n",
    "        # 保存模型\n",
    "#         if epoch % 100 == 0:\n",
    "#             torch.save(state, m)\n",
    "#             print(\"模型{}保存成功\".format(str(epoch)))\n",
    "        \n",
    "        if (epoch == 0 or self.last_loss == 10000):\n",
    "            self.last_loss = loss\n",
    "            self.last_epoch = epoch\n",
    "            torch.save(state, self.p)\n",
    "#             print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "        else:\n",
    "            if (loss <= self.last_loss): #用loss判断最优模型\n",
    "                self.last_loss = loss\n",
    "                self.last_epoch = epoch\n",
    "                torch.save(state, self.p)\n",
    "#                 print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "            else:\n",
    "                pass\n",
    "#                 print(\"best_model未保存，最优模型仍为epoch{}\".format(self.last_epoch,self.last_loss))\n",
    "        \n",
    "    def load(self,optimizer): #TODO\n",
    "\n",
    "        # 如果有保存的模型，则加载模型，并在其基础上继续训练\n",
    "        if os.path.exists(self.p):\n",
    "            checkpoint = torch.load(self.p)\n",
    "            self.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print('加载best_model成功！其为epoch{}'.format(start_epoch))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            print('无保存模型，将从头开始训练！')\n",
    "        return start_epoch\n",
    "\n",
    "    \n",
    "PSP_Rnet = NeuralNetwork()\n",
    "PSP_Rnet = PSP_Rnet.to(device)\n",
    "\n",
    "learning_rate = 2e-2\n",
    "batch_size = 5\n",
    "epochs = 20\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss() #均方误差\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "\n",
    "start_epoch = PSP_Rnet.load(optimizer)\n",
    "\n",
    "# Test the network\n",
    "# TestTensor=torch.Tensor(1,9,1024,512) #数值从0到1，分为9块，每一块中1024行，512列\n",
    "# result = PSP_Rnet(TestTensor)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49057994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class CustomImageDataset(Dataset):#自定义图像数据集\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name =f\"train_data{idx:03d}.npz\"\n",
    "        data_path = os.path.join(self.img_dir, data_name)\n",
    "        data = np.load(data_path)\n",
    "        img1_0, img1_3, img1_4 = data[\"image1\"][0], data[\"image1\"][3], data[\"image1\"][4]\n",
    "        img2_0, img2_1, img2_2 = data[\"image2\"][0], data[\"image2\"][1], data[\"image2\"][2]\n",
    "        img3_0, img3_1, img3_2 = data[\"image3\"][0], data[\"image3\"][1], data[\"image3\"][2]\n",
    "        img1_1, img1_2, img1_5, img1_6, img2_3 = data[\"image1\"][1], data[\"image1\"][2], data[\"image1\"][5], data[\"image1\"][6], data[\"image2\"][3]\n",
    "        \n",
    "        inputs = np.stack([img1_0, img1_3, img1_4, img2_0, img2_1, img2_2, img3_0, img3_1, img3_2]).astype(np.float32)\n",
    "#         outputs = np.stack([img1_1, img1_2, img1_5, img1_6, img2_3]).astype(np.float32)\n",
    "        outputs = np.stack([img1_1, img1_2, img1_5, img1_6]).astype(np.float32)\n",
    "#         print(inputs.shape)\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inputs, outputs\n",
    "    \n",
    "training_data = CustomImageDataset(cfg.net_dir)\n",
    "inputs, outputs = training_data[0]\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34330f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch loss average:1.363411769270897\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch loss average:1.6589682817459106\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch loss average:2.8519087046384812\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch loss average:3.3285215884447097\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch loss average:10.44046893119812\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch loss average:6.893218582868576\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch loss average:7.873975193500518\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch loss average:3.628082752227783\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch loss average:3.142849326133728\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch loss average:2.870594698190689\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch loss average:3.795031708478928\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch loss average:3.2560216784477234\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch loss average:2.148039758205414\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch loss average:1.379454866051674\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch loss average:1.2065574526786804\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch loss average:1.1864033967256546\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch loss average:1.7386541932821273\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch loss average:2.731807056069374\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch loss average:1.238748985528946\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch loss average:2.1265606701374056\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch loss average:4.320678988099099\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch loss average:1.9069073110818864\n",
      "Epoch 41\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b12adbc34772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPSP_Rnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;31m# writer.add_scalar('manner15_gpu_loss',loss, t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#     writer.add_scalars('manner2_gamma_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-b12adbc34772>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, loss_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(\"size : \",size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#batch, (X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# batch = batch.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-156c15047782>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimg2_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimg3_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mimg1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg1_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 return format.read_array(bytes,\n\u001b[0;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"array data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[0;32m    765\u001b[0m                                                              count=read_count)\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[1;34m(fp, size, error_template)\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[1;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 849\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[1;34m(self, newdata)\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m         \u001b[1;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,loss_list):\n",
    "    size = len(dataloader.dataset)\n",
    "#     print(\"size : \",size)\n",
    "    cost = []\n",
    "    for data in enumerate(dataloader):#batch, (X, y)\n",
    "        batch, imgs = data\n",
    "        # batch = batch.to(device)\n",
    "        # imgs = imgs.to(device)\n",
    "        # Compute prediction and loss\n",
    "#         print(\"X.shape : \",X.shape)\n",
    "# #         print(y)\n",
    "# #         print(X)\n",
    "#         print(\"y.shape : \",y.shape)\n",
    "        X,y = imgs\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "#         print(\"batch : \",batch)\n",
    "        model = model.to(device)\n",
    "        pred = model(X)          # 前向传播计算预测值\n",
    "        pred_type = pred.is_cuda\n",
    "        pred = pred.to(device)\n",
    "        loss_fn = loss_fn.to(device)\n",
    "        loss = loss_fn(pred, y)  #真实值与预测值求均方误差\n",
    "#         print(pred-y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # 将模型的参数梯度初始化为0\n",
    "        loss = loss.to(device)\n",
    "        loss.backward()         # 反向传播计算梯度\n",
    "        optimizer.step()        # 更新所有参数\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             model.save()\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        cost.append(loss)\n",
    "        if len(loss_list) < 100//16 + 1:\n",
    "            loss_list.append(loss)\n",
    "#         else:    \n",
    "#             loss_list[batch] = loss\n",
    "            \n",
    "#         print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        PSP_Rnet.save(optimizer,t,loss,batch)\n",
    "    print(f\"Epoch loss average:{np.mean(cost)}\")\n",
    "    return loss\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "if not os.path.exists('./path/to/log'):\n",
    "    os.makedirs('./path/to/log')\n",
    "# writer = SummaryWriter('./path/to/log')\n",
    "epochs = 200000\n",
    "loss_list = []\n",
    "for t in range(start_epoch,epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, PSP_Rnet, loss_fn, optimizer,loss_list)\n",
    "    # writer.add_scalar('manner15_gpu_loss',loss, t)\n",
    "#     writer.add_scalars('manner2_gamma_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\n",
    "    # writer.add_scalars('manner_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6],'batch7':loss_list[7],'batch8':loss_list[8],'batch9':loss_list[9],'batch10':loss_list[10],'batch11':loss_list[11],'batch12':loss_list[12],'batch13':loss_list[13],'batch14':loss_list[14],'batch15':loss_list[15]}, t)\n",
    "#     print('loss_list : ')\n",
    "#     print(loss_list)\n",
    "\n",
    "    sum = 0\n",
    "    element = 0\n",
    "    for i in loss_list:\n",
    "        element += 1\n",
    "        if i <= 0.2:\n",
    "            sum += 1\n",
    "    if sum == element and element > 0:\n",
    "        break\n",
    "#     test_loop(test_dataloader, PSP_Rnet, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
