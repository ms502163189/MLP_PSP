{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55458c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx\n",
      "0th syn data is saved\n",
      "xxxx\n",
      "1th syn data is saved\n",
      "xxxx\n",
      "2th syn data is saved\n",
      "xxxx\n",
      "3th syn data is saved\n",
      "xxxx\n",
      "4th syn data is saved\n",
      "xxxx\n",
      "5th syn data is saved\n",
      "xxxx\n",
      "6th syn data is saved\n",
      "xxxx\n",
      "7th syn data is saved\n",
      "xxxx\n",
      "8th syn data is saved\n",
      "xxxx\n",
      "9th syn data is saved\n",
      "xxxx\n",
      "10th syn data is saved\n",
      "xxxx\n",
      "11th syn data is saved\n",
      "xxxx\n",
      "12th syn data is saved\n",
      "xxxx\n",
      "13th syn data is saved\n",
      "xxxx\n",
      "14th syn data is saved\n",
      "xxxx\n",
      "15th syn data is saved\n",
      "xxxx\n",
      "16th syn data is saved\n",
      "xxxx\n",
      "17th syn data is saved\n",
      "xxxx\n",
      "18th syn data is saved\n",
      "xxxx\n",
      "19th syn data is saved\n",
      "xxxx\n",
      "20th syn data is saved\n",
      "xxxx\n",
      "21th syn data is saved\n",
      "xxxx\n",
      "22th syn data is saved\n",
      "xxxx\n",
      "23th syn data is saved\n",
      "xxxx\n",
      "24th syn data is saved\n",
      "xxxx\n",
      "25th syn data is saved\n",
      "xxxx\n",
      "26th syn data is saved\n",
      "xxxx\n",
      "27th syn data is saved\n",
      "xxxx\n",
      "28th syn data is saved\n",
      "xxxx\n",
      "29th syn data is saved\n",
      "xxxx\n",
      "30th syn data is saved\n",
      "xxxx\n",
      "31th syn data is saved\n",
      "xxxx\n",
      "32th syn data is saved\n",
      "xxxx\n",
      "33th syn data is saved\n",
      "xxxx\n",
      "34th syn data is saved\n",
      "xxxx\n",
      "35th syn data is saved\n",
      "xxxx\n",
      "36th syn data is saved\n",
      "xxxx\n",
      "37th syn data is saved\n",
      "xxxx\n",
      "38th syn data is saved\n",
      "xxxx\n",
      "39th syn data is saved\n",
      "xxxx\n",
      "40th syn data is saved\n",
      "xxxx\n",
      "41th syn data is saved\n",
      "xxxx\n",
      "42th syn data is saved\n",
      "xxxx\n",
      "43th syn data is saved\n",
      "xxxx\n",
      "44th syn data is saved\n",
      "xxxx\n",
      "45th syn data is saved\n",
      "xxxx\n",
      "46th syn data is saved\n",
      "xxxx\n",
      "47th syn data is saved\n",
      "xxxx\n",
      "48th syn data is saved\n",
      "xxxx\n",
      "49th syn data is saved\n",
      "xxxx\n",
      "50th syn data is saved\n",
      "xxxx\n",
      "51th syn data is saved\n",
      "xxxx\n",
      "52th syn data is saved\n",
      "xxxx\n",
      "53th syn data is saved\n",
      "xxxx\n",
      "54th syn data is saved\n",
      "xxxx\n",
      "55th syn data is saved\n",
      "xxxx\n",
      "56th syn data is saved\n",
      "xxxx\n",
      "57th syn data is saved\n",
      "xxxx\n",
      "58th syn data is saved\n",
      "xxxx\n",
      "59th syn data is saved\n",
      "xxxx\n",
      "60th syn data is saved\n",
      "xxxx\n",
      "61th syn data is saved\n",
      "xxxx\n",
      "62th syn data is saved\n",
      "xxxx\n",
      "63th syn data is saved\n",
      "xxxx\n",
      "64th syn data is saved\n",
      "xxxx\n",
      "65th syn data is saved\n",
      "xxxx\n",
      "66th syn data is saved\n",
      "xxxx\n",
      "67th syn data is saved\n",
      "xxxx\n",
      "68th syn data is saved\n",
      "xxxx\n",
      "69th syn data is saved\n",
      "xxxx\n",
      "70th syn data is saved\n",
      "xxxx\n",
      "71th syn data is saved\n",
      "xxxx\n",
      "72th syn data is saved\n",
      "xxxx\n",
      "73th syn data is saved\n",
      "xxxx\n",
      "74th syn data is saved\n",
      "xxxx\n",
      "75th syn data is saved\n",
      "xxxx\n",
      "76th syn data is saved\n",
      "xxxx\n",
      "77th syn data is saved\n",
      "xxxx\n",
      "78th syn data is saved\n",
      "xxxx\n",
      "79th syn data is saved\n",
      "xxxx\n",
      "80th syn data is saved\n",
      "xxxx\n",
      "81th syn data is saved\n",
      "xxxx\n",
      "82th syn data is saved\n",
      "xxxx\n",
      "83th syn data is saved\n",
      "xxxx\n",
      "84th syn data is saved\n",
      "xxxx\n",
      "85th syn data is saved\n",
      "xxxx\n",
      "86th syn data is saved\n",
      "xxxx\n",
      "87th syn data is saved\n",
      "xxxx\n",
      "88th syn data is saved\n",
      "xxxx\n",
      "89th syn data is saved\n",
      "xxxx\n",
      "90th syn data is saved\n",
      "xxxx\n",
      "91th syn data is saved\n",
      "xxxx\n",
      "92th syn data is saved\n",
      "xxxx\n",
      "93th syn data is saved\n",
      "xxxx\n",
      "94th syn data is saved\n",
      "xxxx\n",
      "95th syn data is saved\n",
      "xxxx\n",
      "96th syn data is saved\n",
      "xxxx\n",
      "97th syn data is saved\n",
      "xxxx\n",
      "98th syn data is saved\n",
      "xxxx\n",
      "99th syn data is saved\n"
     ]
    }
   ],
   "source": [
    "# data generator \n",
    "# - using the synthetic images to train our network\n",
    "import os\n",
    "import numpy as np\n",
    "from config import config\n",
    "from fringes import fringe_wrapper\n",
    "from fringes import Fringes_Generator \n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import SummaryWriter   \n",
    "import torch,gc\n",
    "\n",
    "cfg = config()\n",
    "# class Fringes_Generator():\n",
    "#     \"\"\"\n",
    "#     A distributed version of fringes.py. That says, the A is NOT a constant value across the whole image.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config):\n",
    "#         self._c = config\n",
    "#         self.SIG = fringe_wrapper(self._c, \"harmonic\") # synthetic image generator\n",
    "\n",
    "#     def update_cfg(self):\n",
    "#         A0 = np.random.choice(np.linspace(50,150,101)) + np.random.randn()#得到50-150之间一个属\n",
    "#         self._c.A = [np.reshape(np.linspace(A0, 150, self._c.pattern_size[1]), (-1,1))]*3 #生成数组\n",
    "#         A_min, A_max = np.min(self._c.A[0]), np.max(self._c.A[0])\n",
    "#         B_max = np.min([A_min, 255-A_max])\n",
    "#         self._c.B = [(B_max-10)*np.random.rand()+10]*3\n",
    "#         self._c.C = 20*np.random.rand()-10 # -10,10 np.random.rand()返回的是标准正态分布的一个值\n",
    "#         self._c.D = 10*np.random.rand()-5  # -5,5\n",
    "#         self._c.E = 6*np.random.rand()-3   # -3,3\n",
    "#         self._c.F = 4*np.random.rand()-2   # -2,2\n",
    "#         self._c.gamma = 0.8*np.random.rand()+1.5 # 0.7,2.3\n",
    "#         self._c.parameter_array = [A0,self._c.B[0],self._c.C,self._c.D,self._c.E,self._c.F]\n",
    "\n",
    "# #         method = np.random.choice([\"fringe\", \"gamma\", \"harmonic\"])\n",
    "# #         self.SIG = fringe_wrapper(self._c, method) # update the synthetic image generator \n",
    "# #         print(self._c.B[0], self._c.C, self._c.D, self._c.E, self._c.F, self._c.gamma)\n",
    "\n",
    "\n",
    "#     def save_data(self, ind):\n",
    "#         self.update_cfg() # 整个系统重新配置一遍。\n",
    "#         images = self.SIG.generate_all() # 生成图像\n",
    "#         if ind < 20:\n",
    "#             for f, fringes in enumerate(images):\n",
    "#                 for s, fringe in enumerate(fringes):\n",
    "#                     p = Path(self._c.pattern_path)/f\"{ind:0>3d}{f:0>2d}{s:0>2d}{self._c.hv}.bmp\" \n",
    "#                     cv2.imwrite(str(p), fringe)\n",
    "#         np.savez(self._c.net_dir+f\"train_data{ind:03d}\", image1=images[0], image2=images[1], image3=images[2]) #将3个条纹保存在未压缩的.npz格式中\n",
    "#         print(f\"{ind}th syn data is saved\")\n",
    "cfg.net_dir=\"./data/train_gamma/\"\n",
    "cfg.pattern_size = [1920, 50] #[1920, 50] [512, 1]\n",
    "cfg.Tp = [30, 33, 36]       #在这个实验中，就先设置成这个周期。\n",
    "cfg.steps = [7,4,3]\n",
    "cfg.gamma = 1.0\n",
    "\n",
    "#删除文件\n",
    "def del_files(path_file):\n",
    "    ls = os.listdir(path_file)\n",
    "    for i in ls:\n",
    "        f_path = os.path.join(path_file, i)\n",
    "        # 判断是否是一个目录,若是,则递归删除\n",
    "        if os.path.isdir(f_path):\n",
    "            del_files(f_path)\n",
    "        else:\n",
    "            os.remove(f_path)\n",
    "\n",
    "a = input(\"是否需要重新生成数据集，请输入Y或者N ：\")\n",
    "if a == 'Y':\n",
    "    cfg.pattern_path = './data/fringe_gamma'\n",
    "\n",
    "    if not os.path.exists(cfg.pattern_path):\n",
    "        os.makedirs(cfg.pattern_path)\n",
    "    if not os.path.exists(cfg.net_dir):\n",
    "        os.makedirs(cfg.net_dir)\n",
    "\n",
    "    del_files(cfg.pattern_path+\"/\")\n",
    "    del_files(cfg.net_dir)\n",
    "    # for i in range(100):\n",
    "        # images = fringe_wrapper(cfg, \"gamma\").generate_all()\n",
    "        # np.savez(f\"./data/train_gamma/train_data{i:03d}\", image1=images[0], image2=images[1], image3=images[2]) \n",
    "    for ind in range(100):\n",
    "        fringe_generator=Fringes_Generator(cfg)\n",
    "        fringe_generator.save_data(ind)\n",
    "\n",
    "\n",
    "b = input(\"是否需要重新开始训练，请输入Y或者N ：\")\n",
    "if b == 'Y':\n",
    "    if not os.path.exists(\"./data/model_gamma\"):\n",
    "        os.makedirs(\"./data/model_gamma\")\n",
    "    del_files(\"./data/model_gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0530e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.C :  tensor([[ 0.8019,  0.3569,  0.3569,  0.8019],\n",
      "        [ 1.0000,  1.4450, -0.8019, -0.8019],\n",
      "        [-0.8019, -0.8019,  1.4450,  1.0000]], device='cuda:0')\n",
      "无保存模型，将从头开始训练！\n"
     ]
    }
   ],
   "source": [
    "# build a MLP model for residual estimation \n",
    "#- The linear part is also considered in this network\n",
    "from torch import nn\n",
    "\n",
    "# 定义训练的设备\n",
    "device = torch.device(\"cuda\") #使用gpu进行训练\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()#清楚cuda缓存\n",
    "\n",
    "class NeuralNetwork(nn.Module): #构建CNN神经网络\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.MLP1 = nn.Sequential(\n",
    "            nn.Conv2d(9,  64, 1, stride=1),  #四个卷积层\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(64, 256, 1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(256, 128, 1, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(128, 4, 1, stride=1),\n",
    "        )\n",
    "        # self.MLP2 = nn.Sequential(\n",
    "        #     nn.Conv2d(9,  64, 1, stride=1),  #四个卷积层\n",
    "        #     nn.PReLU(),\n",
    "        #     nn.Conv2d(64, 128, 1, stride=1),\n",
    "        #     nn.PReLU(),\n",
    "        #     nn.Conv2d(128, 4, 1, stride=1),\n",
    "        # )\n",
    "\n",
    "#         self.resnet =  nn.Sequential(\n",
    "#             nn.Conv2d(9, 5, 1, stride=1),\n",
    "#             nn.BatchNorm2d(5)\n",
    "#             # nn.ReLU()   \n",
    "#         )\n",
    "        \n",
    "        A = [[1, np.cos(0), np.sin(0)],                \n",
    "             [1, np.cos(6*np.pi/7), np.sin(6*np.pi/7)],\n",
    "             [1, np.cos(8*np.pi/7), np.sin(8*np.pi/7)]]\n",
    "        B = [[1, np.cos(2*np.pi/7), np.sin(2*np.pi/7)],\n",
    "             [1, np.cos(4*np.pi/7), np.sin(4*np.pi/7)],\n",
    "             [1, np.cos(10*np.pi/7), np.sin(10*np.pi/7)],\n",
    "             [1, np.cos(12*np.pi/7), np.sin(12*np.pi/7)]]\n",
    "        A, B = np.array(A), np.array(B) #列表转化成数组\n",
    "        self.C = torch.Tensor(np.matmul(B, np.linalg.inv(A)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.C = self.C.to(device)\n",
    "        print(\"self.C : \",self.C)\n",
    "        D = [[1, np.cos(0), np.sin(0)],\n",
    "            [1, np.cos(2*np.pi/4), np.sin(2*np.pi/4)],\n",
    "            [1, np.cos(4*np.pi/4), np.sin(4*np.pi/4)]]\n",
    "        E = [[1, np.cos(6*np.pi/4), np.sin(6*np.pi/4)]]\n",
    "        self.F = torch.Tensor(np.matmul(E, np.linalg.pinv(D)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.F = self.F.to(device)\n",
    "        self.model_save_dir = \"./data/model_gamma\" #定义保存模型的文件夹\n",
    "        self.p = os.path.join(self.model_save_dir,\"best_model\")\n",
    "        self.last_loss = 9999999999\n",
    "        self.last_epoch = 0\n",
    "        \n",
    "        #对数组A求逆矩阵，与B矩阵相乘\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # linear generator for the 4 images\n",
    "        images = images.to(device)\n",
    "        i7s = images.permute(0, 2, 3,1)            #将tensor的维度换位\n",
    "        # i7s = i7s.to(device)\n",
    "        i7s,_ = torch.split(i7s,(3,6),dim=-1)      #切分，得到的就是I1 I7 I8\n",
    "#         mids,another = torch.split(_,(3,3),dim = -1) #切分，得到4步相移的前三步\n",
    "\n",
    "        linearPart = torch.matmul(i7s, self.C+0.0) #Y矩阵乘法 50*(1950*3)矩阵乘以3*4矩阵\n",
    "#         linearPart1 = torch.matmul(mids, self.F+0.0)   #矩阵乘法 50*(1950*3)矩阵乘以3*1矩阵\n",
    "        linearPart = linearPart.to(device)\n",
    "#         linearPart1 = linearPart1.to(device)\n",
    "        linearPart = linearPart.permute(0,3,1,2)   #维度换位\n",
    "#         linearPart1 = linearPart1.permute(0,3,1,2)   #维度换位\n",
    "#         linearPart2 = torch.cat([linearPart, linearPart1], 1)\n",
    "        \n",
    "        # The nonliear part is estimated with MLP function \n",
    "        images = images/255\n",
    "        images = images.to(device)\n",
    "        res1 = self.MLP1(images)\n",
    "        # res2 = self.MLP2(images)\n",
    "#         out1 = res1 + self.resnet(images)\n",
    "        # out = self.resnet1(out1)\n",
    "        # out = self.conv2(out1)\n",
    "        # res2 = self.conv2(out1)\n",
    "        # out = res2 + self.resnet2(out1)\n",
    "        # print_out = nn.ReLU(out)\n",
    "        # print(\"res.shape:{}\".format(res.shape))\n",
    "        # RES = res.view(-1,64*cfg.pattern_size[1]*cfg.pattern_size[0])\n",
    "        # print(\"RES.shape:{}\".format(RES.shape))\n",
    "        # RES = self.classifier(RES)\n",
    "        # RES1 = RES.view(len(RES),-1,cfg.pattern_size[0])\n",
    "        # print(\"RES1.shape:{}\".format(RES1.shape))\n",
    "        # RES2 = RES1.view(len(RES),-1,cfg.pattern_size[1],cfg.pattern_size[0])\n",
    "        # print(\"RES2.shape:{}\".format(RES2.shape))\n",
    "        \n",
    "#         print(\"zhixingle\")\n",
    "        return linearPart + res1#+res2\n",
    "    \n",
    "    def save(self,optimizer,epoch,loss,batch): #TODO\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        m = os.path.join(self.model_save_dir,\"model\"+str(epoch))\n",
    "\n",
    "        state = {'model':self.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n",
    "        # 保存模型\n",
    "#         if epoch % 100 == 0:\n",
    "#             torch.save(state, m)\n",
    "#             print(\"模型{}保存成功\".format(str(epoch)))\n",
    "        \n",
    "        if (epoch == 0 or self.last_loss == 10000):\n",
    "            self.last_loss = loss\n",
    "            self.last_epoch = epoch\n",
    "            torch.save(state, self.p)\n",
    "#             print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "        else:\n",
    "            if (loss <= self.last_loss): #用loss判断最优模型\n",
    "                self.last_loss = loss\n",
    "                self.last_epoch = epoch\n",
    "                torch.save(state, self.p)\n",
    "#                 print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "            else:\n",
    "                pass\n",
    "#                 print(\"best_model未保存，最优模型仍为epoch{}\".format(self.last_epoch,self.last_loss))\n",
    "        \n",
    "    def load(self,optimizer): #TODO\n",
    "\n",
    "        # 如果有保存的模型，则加载模型，并在其基础上继续训练\n",
    "        if os.path.exists(self.p):\n",
    "            checkpoint = torch.load(self.p)\n",
    "            self.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print('加载best_model成功！其为epoch{}'.format(start_epoch))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            print('无保存模型，将从头开始训练！')\n",
    "        return start_epoch\n",
    "\n",
    "    \n",
    "PSP_Rnet = NeuralNetwork()\n",
    "PSP_Rnet = PSP_Rnet.to(device)\n",
    "\n",
    "learning_rate = 2e-2\n",
    "batch_size = 5\n",
    "epochs = 20\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss() #均方误差\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "\n",
    "start_epoch = PSP_Rnet.load(optimizer)\n",
    "\n",
    "# Test the network\n",
    "# TestTensor=torch.Tensor(1,9,1024,512) #数值从0到1，分为9块，每一块中1024行，512列\n",
    "# result = PSP_Rnet(TestTensor)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49057994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class CustomImageDataset(Dataset):#自定义图像数据集\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name =f\"train_data{idx:03d}.npz\"\n",
    "        data_path = os.path.join(self.img_dir, data_name)\n",
    "        data = np.load(data_path)\n",
    "        img1_0, img1_3, img1_4 = data[\"image1\"][0], data[\"image1\"][3], data[\"image1\"][4]\n",
    "        img2_0, img2_1, img2_2 = data[\"image2\"][0], data[\"image2\"][1], data[\"image2\"][2]\n",
    "        img3_0, img3_1, img3_2 = data[\"image3\"][0], data[\"image3\"][1], data[\"image3\"][2]\n",
    "        img1_1, img1_2, img1_5, img1_6, img2_3 = data[\"image1\"][1], data[\"image1\"][2], data[\"image1\"][5], data[\"image1\"][6], data[\"image2\"][3]\n",
    "        \n",
    "        inputs = np.stack([img1_0, img1_3, img1_4, img2_0, img2_1, img2_2, img3_0, img3_1, img3_2]).astype(np.float32)\n",
    "#         outputs = np.stack([img1_1, img1_2, img1_5, img1_6, img2_3]).astype(np.float32)\n",
    "        outputs = np.stack([img1_1, img1_2, img1_5, img1_6]).astype(np.float32)\n",
    "#         print(inputs.shape)\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inputs, outputs\n",
    "    \n",
    "training_data = CustomImageDataset(cfg.net_dir)\n",
    "inputs, outputs = training_data[0]\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34330f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Epoch loss average:16.08298816680908\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch loss average:10.168757271766662\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch loss average:3.4333618104457857\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch loss average:1.7327015221118927\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch loss average:1.331419548392296\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch loss average:0.862777179479599\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch loss average:0.8076912105083466\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch loss average:0.5766705751419068\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch loss average:0.51349428743124\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch loss average:0.41067678183317186\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch loss average:0.5097693875432014\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch loss average:0.4871143952012062\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch loss average:0.4536628097295761\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch loss average:1.6433885455131532\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch loss average:0.978622068464756\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1449d80acd40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPSP_Rnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;31m# writer.add_scalar('manner15_gpu_loss',loss, t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#     writer.add_scalars('manner2_gamma_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-1449d80acd40>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, loss_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(\"size : \",size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#batch, (X, y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# batch = batch.to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-156c15047782>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mimg1_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg1_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mimg2_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mimg3_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"image3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 return format.read_array(bytes,\n\u001b[0;32m    254\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"array data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m--> 765\u001b[1;33m                                                              count=read_count)\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfortran_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,loss_list):\n",
    "    size = len(dataloader.dataset)\n",
    "#     print(\"size : \",size)\n",
    "    cost = []\n",
    "    for data in enumerate(dataloader):#batch, (X, y)\n",
    "        batch, imgs = data\n",
    "        # batch = batch.to(device)\n",
    "        # imgs = imgs.to(device)\n",
    "        # Compute prediction and loss\n",
    "#         print(\"X.shape : \",X.shape)\n",
    "# #         print(y)\n",
    "# #         print(X)\n",
    "#         print(\"y.shape : \",y.shape)\n",
    "        X,y = imgs\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "#         print(\"batch : \",batch)\n",
    "        model = model.to(device)\n",
    "        pred = model(X)          # 前向传播计算预测值\n",
    "        pred_type = pred.is_cuda\n",
    "        pred = pred.to(device)\n",
    "        loss_fn = loss_fn.to(device)\n",
    "        loss = loss_fn(pred, y)  #真实值与预测值求均方误差\n",
    "#         print(pred-y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # 将模型的参数梯度初始化为0\n",
    "        loss = loss.to(device)\n",
    "        loss.backward()         # 反向传播计算梯度\n",
    "        optimizer.step()        # 更新所有参数\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             model.save()\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        cost.append(loss)\n",
    "        if len(loss_list) < 100//16 + 1:\n",
    "            loss_list.append(loss)\n",
    "#         else:    \n",
    "#             loss_list[batch] = loss\n",
    "            \n",
    "#         print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        PSP_Rnet.save(optimizer,t,loss,batch)\n",
    "    print(f\"Epoch loss average:{np.mean(cost)}\")\n",
    "    return loss\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "if not os.path.exists('./path/to/log'):\n",
    "    os.makedirs('./path/to/log')\n",
    "# writer = SummaryWriter('./path/to/log')\n",
    "# epochs = 50\n",
    "loss_list = []\n",
    "for t in range(start_epoch,epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, PSP_Rnet, loss_fn, optimizer,loss_list)\n",
    "    # writer.add_scalar('manner15_gpu_loss',loss, t)\n",
    "#     writer.add_scalars('manner2_gamma_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\n",
    "    # writer.add_scalars('manner_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6],'batch7':loss_list[7],'batch8':loss_list[8],'batch9':loss_list[9],'batch10':loss_list[10],'batch11':loss_list[11],'batch12':loss_list[12],'batch13':loss_list[13],'batch14':loss_list[14],'batch15':loss_list[15]}, t)\n",
    "#     print('loss_list : ')\n",
    "#     print(loss_list)\n",
    "\n",
    "    sum = 0\n",
    "    element = 0\n",
    "    for i in loss_list:\n",
    "        element += 1\n",
    "        if i <= 0.2:\n",
    "            sum += 1\n",
    "    if sum == element and element > 0:\n",
    "        break\n",
    "#     test_loop(test_dataloader, PSP_Rnet, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
