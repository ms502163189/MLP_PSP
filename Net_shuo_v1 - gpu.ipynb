{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator \n",
    "# - using the synthetic images to train our network\n",
    "import os\n",
    "import numpy as np\n",
    "from config import config\n",
    "from fringes import fringe_wrapper\n",
    "from fringes import Fringes_Generator \n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import torch,gc\n",
    "\n",
    "cfg = config()\n",
    "cfg.net_dir=\"./data/train3/\"\n",
    "cfg.model_dir = \"./data/model3/\"\n",
    "cfg.tensorboard_dir = \"./path/to/log/\"\n",
    "cfg.pattern_size = [1920, 50] #[1920, 50] [512, 1]\n",
    "cfg.Tp = [30, 33, 36]       #在这个实验中，就先设置成这个周期。\n",
    "cfg.steps = [7,4,3]\n",
    "cfg.gamma = 1.1\n",
    "\n",
    "#删除文件\n",
    "def del_files(path_file):\n",
    "    ls = os.listdir(path_file)\n",
    "    for i in ls:\n",
    "        f_path = os.path.join(path_file, i)\n",
    "        # 判断是否是一个目录,若是,则递归删除\n",
    "        if os.path.isdir(f_path):\n",
    "            del_files(f_path)\n",
    "        else:\n",
    "            os.remove(f_path)\n",
    "\n",
    "a = input(\"是否需要重新生成数据集，请输入Y或者N ：\")\n",
    "if a == 'Y':\n",
    "    if not os.path.exists(cfg.net_dir):\n",
    "        os.makedirs(cfg.net_dir)\n",
    "    del_files(cfg.net_dir)\n",
    "    # #gamma\n",
    "    # for i in range(100):\n",
    "    #     images = fringe_wrapper(cfg, \"gamma\").generate_all()\n",
    "    #     np.savez(f\"./data/train3/train_data{i:03d}\", image1=images[0], image2=images[1], image3=images[2]) \n",
    "\n",
    "    # #harmonic\n",
    "    for ind in range(100):\n",
    "        print(ind)\n",
    "        fringe_generator=Fringes_Generator(cfg)\n",
    "        fringe_generator.save_data(ind)\n",
    "\n",
    "\n",
    "\n",
    "b = input(\"是否需要重新开始训练，请输入Y或者N ：\")\n",
    "if b == 'Y':\n",
    "    if not os.path.exists(cfg.model_dir):\n",
    "        os.makedirs(cfg.model_dir)\n",
    "    del_files(cfg.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.C :  tensor([[ 0.8019,  0.3569,  0.3569,  0.8019],\n",
      "        [ 1.0000,  1.4450, -0.8019, -0.8019],\n",
      "        [-0.8019, -0.8019,  1.4450,  1.0000]], device='cuda:0')\n",
      "无保存模型，将从头开始训练！\n"
     ]
    }
   ],
   "source": [
    "# build a MLP model for residual estimation \n",
    "#- The linear part is also considered in this network\n",
    "from torch import nn\n",
    "\n",
    "# 定义训练的设备\n",
    "device = torch.device(\"cuda\") #使用gpu进行训练\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()#清楚cuda缓存\n",
    "\n",
    "class NeuralNetwork(nn.Module): #构建CNN神经网络\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Conv2d(9,  64, 1, stride=1),  #四个卷积层\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 256, 1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 4, 1, stride=1),\n",
    "        )\n",
    "        \n",
    "        A = [[1, np.cos(0), np.sin(0)],                \n",
    "             [1, np.cos(6*np.pi/7), np.sin(6*np.pi/7)],\n",
    "             [1, np.cos(8*np.pi/7), np.sin(8*np.pi/7)]]\n",
    "        B = [[1, np.cos(2*np.pi/7), np.sin(2*np.pi/7)],\n",
    "             [1, np.cos(4*np.pi/7), np.sin(4*np.pi/7)],\n",
    "             [1, np.cos(10*np.pi/7), np.sin(10*np.pi/7)],\n",
    "             [1, np.cos(12*np.pi/7), np.sin(12*np.pi/7)]]\n",
    "        A, B = np.array(A), np.array(B) #列表转化成数组\n",
    "        self.C = torch.Tensor(np.matmul(B,np.linalg.inv(A)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.C = self.C.to(device)\n",
    "        print(\"self.C : \",self.C)\n",
    "        D = [[1, np.cos(0), np.sin(0)],\n",
    "            [1, np.cos(2*np.pi/4), np.sin(2*np.pi/4)],\n",
    "            [1, np.cos(4*np.pi/4), np.sin(4*np.pi/4)]]\n",
    "        E = [[1, np.cos(6*np.pi/4), np.sin(6*np.pi/4)]]\n",
    "        self.F = torch.Tensor(np.matmul(E, np.linalg.pinv(D)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.F = self.F.to(device)\n",
    "        self.model_save_dir = \"./data/model3\" #定义保存模型的文件夹\n",
    "        self.p = os.path.join(self.model_save_dir,\"best_model\")\n",
    "        self.last_loss = 9999999999\n",
    "        self.last_epoch = 0\n",
    "        \n",
    "        #对数组A求逆矩阵，与B矩阵相乘\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # linear generator for the 4 images\n",
    "        images = images.to(device)\n",
    "        i7s = images.permute(0, 2, 3,1)            #将tensor的维度换位\n",
    "        # i7s = i7s.to(device)\n",
    "        i7s,_ = torch.split(i7s,(3,6),dim=-1)      #切分，得到的就是I1 I7 I8\n",
    "        # mids,another = torch.split(_,(3,3),dim = -1) #切分，得到4步相移的前三步\n",
    "        # print(\"self.C.shape\",self.C.shape)\n",
    "        # print(\"i7s.shape\",i7s.shape)\n",
    "\n",
    "        linearPart = torch.matmul(i7s, self.C+0.0) #Y矩阵乘法 50*(1950*3)矩阵乘以3*4矩阵\n",
    "        # linearPart1 = torch.matmul(mids, self.F+0.0)   #矩阵乘法 50*(1950*3)矩阵乘以3*1矩阵\n",
    "        linearPart = linearPart.to(device)\n",
    "        # linearPart1 = linearPart1.to(device)\n",
    "        linearPart = linearPart.permute(0,3,1,2)   #维度换位\n",
    "        # linearPart1 = linearPart1.permute(0,3,1,2)   #维度换位\n",
    "        # linearPart2 = torch.cat([linearPart, linearPart1], 1)\n",
    "        \n",
    "        # The nonliear part is estimated with MLP function \n",
    "        images = images.to(device)\n",
    "        images = images/255\n",
    "        images = images.to(device)\n",
    "        res = self.MLP(images)\n",
    "        return linearPart + res\n",
    "    \n",
    "    def save(self,optimizer,epoch,loss,batch): #TODO\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        m = os.path.join(self.model_save_dir,\"model\"+str(epoch))\n",
    "\n",
    "        state = {'model':self.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n",
    "        # 保存模型\n",
    "#         if epoch % 100 == 0:\n",
    "#             torch.save(state, m)\n",
    "#             print(\"模型{}保存成功\".format(str(epoch)))\n",
    "        \n",
    "        if (epoch == 0 or self.last_loss == 10000):\n",
    "            self.last_loss = loss\n",
    "            self.last_epoch = epoch\n",
    "            torch.save(state, self.p)\n",
    "            # print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "        else:\n",
    "            if (loss <= self.last_loss): #用loss判断最优模型\n",
    "                self.last_loss = loss\n",
    "                self.last_epoch = epoch\n",
    "                torch.save(state, self.p)\n",
    "                # print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "            else:\n",
    "                pass\n",
    "                # print(\"best_model未保存，最优模型仍为epoch{}\".format(self.last_epoch,self.last_loss))\n",
    "        \n",
    "    def load(self,optimizer): #TODO\n",
    "\n",
    "        # 如果有保存的模型，则加载模型，并在其基础上继续训练\n",
    "        if os.path.exists(self.p):\n",
    "            checkpoint = torch.load(self.p)\n",
    "            self.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print('加载best_model成功！其为epoch{}'.format(start_epoch))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            print('无保存模型，将从头开始训练！')\n",
    "        return start_epoch\n",
    "\n",
    "    \n",
    "PSP_Rnet = NeuralNetwork()\n",
    "PSP_Rnet = PSP_Rnet.to(device)\n",
    "\n",
    "learning_rate = 2e-2\n",
    "batch_size = 5\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss() #均方误差\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "start_epoch = PSP_Rnet.load(optimizer)\n",
    "\n",
    "# Test the network\n",
    "# TestTensor=torch.Tensor(1,9,1024,512) #数值从0到1，分为9块，每一块中1024行，512列\n",
    "# result = PSP_Rnet(TestTensor)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class CustomImageDataset(Dataset):#自定义图像数据集\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name =f\"train_data{idx:03d}.npz\"\n",
    "        data_path = os.path.join(self.img_dir, data_name)\n",
    "        data = np.load(data_path)\n",
    "        img1_0, img1_3, img1_4 = data[\"image1\"][0], data[\"image1\"][3], data[\"image1\"][4]\n",
    "        img2_0, img2_1, img2_2 = data[\"image2\"][0], data[\"image2\"][1], data[\"image2\"][2]\n",
    "        img3_0, img3_1, img3_2 = data[\"image3\"][0], data[\"image3\"][1], data[\"image3\"][2]\n",
    "        img1_1, img1_2, img1_5, img1_6, img2_3 = data[\"image1\"][1], data[\"image1\"][2], data[\"image1\"][5], data[\"image1\"][6], data[\"image2\"][3]\n",
    "        inputs = np.stack([img1_0, img1_3, img1_4, img2_0, img2_1, img2_2, img3_0, img3_1, img3_2]).astype(np.float32)\n",
    "        outputs = np.stack([img1_1, img1_2, img1_5, img1_6]).astype(np.float32)\n",
    "\n",
    "\n",
    "        # img11_0, img11_3, img11_4 = data[\"image4\"][0], data[\"image4\"][3], data[\"image4\"][4]\n",
    "        # img22_0, img22_1, img22_2 = data[\"image5\"][0], data[\"image5\"][1], data[\"image5\"][2]\n",
    "        # img33_0, img33_1, img33_2 = data[\"image6\"][0], data[\"image6\"][1], data[\"image6\"][2]\n",
    "\n",
    "        # inputs1 = np.stack([img11_0, img11_3, img11_4, img22_0, img22_1, img22_2, img33_0, img33_1, img33_2]).astype(np.float32)\n",
    "#         print(inputs.shape)\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inputs,outputs\n",
    "    \n",
    "training_data = CustomImageDataset(cfg.net_dir)\n",
    "inputs, outputs = training_data[0]\n",
    "# print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4fe8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer,loss_list):\n",
    "    cost = []\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(\"size : \",size)\n",
    "    for data in enumerate(dataloader):#batch, (X, y)\n",
    "        batch, imgs = data\n",
    "        # batch = batch.to(device)\n",
    "        # imgs = imgs.to(device)\n",
    "        # Compute prediction and loss\n",
    "#         print(\"X.shape : \",X.shape)\n",
    "# #         print(y)\n",
    "# #         print(X)\n",
    "#         print(\"y.shape : \",y.shape)\n",
    "        X,y = imgs\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # print(\"batch : \",batch)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        pred = model(X)          # 前向传播计算预测值\n",
    "        # pred_type = pred.is_cuda\n",
    "        pred = pred.to(device)\n",
    "        loss_fn = loss_fn.to(device)\n",
    "        loss = loss_fn(pred, y)  #真实值与预测值求均方误差\n",
    "        # print(\"pred\",pred)\n",
    "        # print(\"y\",y)\n",
    "#         print(pred-y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # 将模型的参数梯度初始化为0\n",
    "        loss = loss.to(device)\n",
    "        loss.backward()         # 反向传播计算梯度\n",
    "        optimizer.step()        # 更新所有参数\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             model.save()\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        cost.append(loss)\n",
    "        if len(loss_list) < 100//batch_size:\n",
    "            loss_list.append(loss)\n",
    "        else:    \n",
    "            loss_list[batch] = loss\n",
    "            \n",
    "        # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        PSP_Rnet.save(optimizer,t,loss,batch)\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    print(f\"Epoch loss average:{np.mean(cost)}\")\n",
    "    return np.mean(cost)\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "if not os.path.exists(cfg.tensorboard_dir):\n",
    "    os.makedirs(cfg.tensorboard_dir)\n",
    "del_files(cfg.tensorboard_dir)\n",
    "writer = SummaryWriter(cfg.tensorboard_dir)\n",
    "epochs = 200000\n",
    "loss_list = []\n",
    "for t in range(start_epoch,epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, PSP_Rnet, loss_fn, optimizer,loss_list)\n",
    "    scheduler.step() # 需要在优化器参数更新之后再动态调整学习率\n",
    "    \n",
    "    writer.add_scalar('manner3_gpu_loss',loss, t)\n",
    "    # writer.add_scalars('manner1_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\n",
    "    sum = 0\n",
    "    element = 0\n",
    "    for i in loss_list:\n",
    "        element += 1\n",
    "        if i <= 0.2:\n",
    "            sum += 1\n",
    "    if sum == element and element > 0:\n",
    "        break\n",
    "#     test_loop(test_dataloader, PSP_Rnet, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e549dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
