{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator \n",
    "# - using the synthetic images to train our network\n",
    "import os\n",
    "import numpy as np\n",
    "from config import config\n",
    "from fringes import fringe_wrapper\n",
    "from fringes import Fringes_Generator \n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import torch,gc\n",
    "\n",
    "cfg = config()\n",
    "cfg.net_dir=\"./data/train3/\"\n",
    "cfg.model_dir = \"./data/model3/\"\n",
    "cfg.tensorboard_dir = \"./path/to/log/\"\n",
    "cfg.pattern_size = [1920, 50] #[1920, 50] [512, 1]\n",
    "cfg.Tp = [30, 33, 36]       #在这个实验中，就先设置成这个周期。\n",
    "cfg.steps = [7,4,3]\n",
    "# cfg.gamma = 1.4\n",
    "\n",
    "#删除文件\n",
    "def del_files(path_file):\n",
    "    ls = os.listdir(path_file)\n",
    "    for i in ls:\n",
    "        f_path = os.path.join(path_file, i)\n",
    "        # 判断是否是一个目录,若是,则递归删除\n",
    "        if os.path.isdir(f_path):\n",
    "            del_files(f_path)\n",
    "        else:\n",
    "            os.remove(f_path)\n",
    "\n",
    "a = input(\"是否需要重新生成数据集，请输入Y或者N ：\")\n",
    "if a == 'Y':\n",
    "    if not os.path.exists(cfg.net_dir):\n",
    "        os.makedirs(cfg.net_dir)\n",
    "    del_files(cfg.net_dir)\n",
    "    # #gamma\n",
    "    num = input(\"需要生成的数据集数量\")\n",
    "    for i in range(int(num)):\n",
    "        cfg.gamma = random.uniform(0.8,2.0)\n",
    "        # print(cfg.gamma)\n",
    "        images = fringe_wrapper(cfg, \"gamma\").generate_all()\n",
    "        np.savez(f\"./data/train3/train_data{i:03d}\", image1=images[0], image2=images[1], image3=images[2]) \n",
    "\n",
    "    # #harmonic\n",
    "    # for ind in range(100):\n",
    "    #     print(ind)\n",
    "    #     fringe_generator=Fringes_Generator(cfg)\n",
    "    #     fringe_generator.save_data(ind)\n",
    "\n",
    "\n",
    "\n",
    "b = input(\"是否需要重新开始训练，请输入Y或者N ：\")\n",
    "if b == 'Y':\n",
    "    if not os.path.exists(cfg.model_dir):\n",
    "        os.makedirs(cfg.model_dir)\n",
    "    del_files(cfg.model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.C :  tensor([[ 0.8019,  0.3569,  0.3569,  0.8019],\n",
      "        [ 1.0000,  1.4450, -0.8019, -0.8019],\n",
      "        [-0.8019, -0.8019,  1.4450,  1.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# build a MLP model for residual estimation \n",
    "#- The linear part is also considered in this network\n",
    "from torch import nn\n",
    "from MLP_net import NeuralNetwork\n",
    "\n",
    "# 定义训练的设备\n",
    "device = torch.device(\"cuda\") #使用gpu进行训练\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()#清楚cuda缓存\n",
    "\n",
    "# 定义模型及参数\n",
    "PSP_Rnet = NeuralNetwork(cfg)\n",
    "PSP_Rnet = PSP_Rnet.to(device)\n",
    "\n",
    "# Initialize the learning rate\n",
    "learning_rate = 2e-2\n",
    "batch_size = 5\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss() #均方误差\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "# dynamic learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class CustomImageDataset(Dataset):#自定义图像数据集\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name =f\"train_data{idx:03d}.npz\"\n",
    "        data_path = os.path.join(self.img_dir, data_name)\n",
    "        data = np.load(data_path)\n",
    "        img1_0, img1_3, img1_4 = data[\"image1\"][0], data[\"image1\"][3], data[\"image1\"][4]\n",
    "        img2_0, img2_1, img2_2 = data[\"image2\"][0], data[\"image2\"][1], data[\"image2\"][2]\n",
    "        img3_0, img3_1, img3_2 = data[\"image3\"][0], data[\"image3\"][1], data[\"image3\"][2]\n",
    "        img1_1, img1_2, img1_5, img1_6, img2_3 = data[\"image1\"][1], data[\"image1\"][2], data[\"image1\"][5], data[\"image1\"][6], data[\"image2\"][3]\n",
    "        inputs = np.stack([img1_0, img1_3, img1_4, img2_0, img2_1, img2_2, img3_0, img3_1, img3_2]).astype(np.float32)\n",
    "        outputs = np.stack([img1_1, img1_2, img1_5, img1_6]).astype(np.float32)\n",
    "\n",
    "\n",
    "        # img11_0, img11_3, img11_4 = data[\"image4\"][0], data[\"image4\"][3], data[\"image4\"][4]\n",
    "        # img22_0, img22_1, img22_2 = data[\"image5\"][0], data[\"image5\"][1], data[\"image5\"][2]\n",
    "        # img33_0, img33_1, img33_2 = data[\"image6\"][0], data[\"image6\"][1], data[\"image6\"][2]\n",
    "\n",
    "        # inputs1 = np.stack([img11_0, img11_3, img11_4, img22_0, img22_1, img22_2, img33_0, img33_1, img33_2]).astype(np.float32)\n",
    "#         print(inputs.shape)\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inputs,outputs\n",
    "    \n",
    "training_data = CustomImageDataset(cfg.net_dir)\n",
    "inputs, outputs = training_data[0]\n",
    "# print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4fe8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer,epoch):\n",
    "    cost = []\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(\"size : \",size)\n",
    "    for data in enumerate(dataloader):#batch, (X, y)\n",
    "        batch, imgs = data\n",
    "        X,y = imgs\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        pred = model(X)          # 前向传播计算预测值\n",
    "        pred = pred.to(device)\n",
    "        loss_fn = loss_fn.to(device)\n",
    "        loss = loss_fn(pred, y)  #真实值与预测值求均方误差\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # 将模型的参数梯度初始化为0\n",
    "        loss = loss.to(device)\n",
    "        loss.backward()         # 反向传播计算梯度\n",
    "        optimizer.step()        # 更新所有参数\n",
    "\n",
    "        loss = loss.item()\n",
    "        cost.append(loss)\n",
    "        # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        model.save(optimizer,epoch,loss,batch)\n",
    "    print(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "    print(f\"Epoch loss average:{np.mean(cost)}\")\n",
    "    return np.mean(cost)\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a29b7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无保存模型，将从头开始训练！\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "0.02\n",
      "Epoch loss average:53.54123436272144\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "0.02\n",
      "Epoch loss average:2.957446837425232\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "0.002\n",
      "Epoch loss average:0.937663272023201\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "0.002\n",
      "Epoch loss average:0.8603253802657127\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "0.0002\n",
      "Epoch loss average:0.8114617019891739\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "0.0002\n",
      "Epoch loss average:0.802739852964878\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "2e-05\n",
      "Epoch loss average:0.7966319292783737\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "2e-05\n",
      "Epoch loss average:0.795425725877285\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './data/model3/best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eca0582c1462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPSP_Rnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 需要在优化器参数更新之后再动态调整学习率\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ddf31d195df1>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param_groups'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch loss average:{np.mean(cost)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\mlp\\MLP_PSP\\MLP_net.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, optimizer, epoch, loss, batch)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './data/model3/best_model'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(cfg.tensorboard_dir):\n",
    "    os.makedirs(cfg.tensorboard_dir)\n",
    "del_files(cfg.tensorboard_dir)\n",
    "writer = SummaryWriter(cfg.tensorboard_dir) #使用tensorboard查看loss\n",
    "\n",
    "epochs = 200000\n",
    "\n",
    "start_epoch = PSP_Rnet.load(optimizer)\n",
    "for t in range(start_epoch,epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, PSP_Rnet, loss_fn, optimizer,t)\n",
    "\n",
    "    if optimizer.state_dict()['param_groups'][0]['lr'] == 0.02:\n",
    "        scheduler.step() # 需要在优化器参数更新之后再动态调整学习率\n",
    "    \n",
    "    writer.add_scalar('manner3_gpu_loss',loss, t)\n",
    "\n",
    "\n",
    "    if loss < 0.5:\n",
    "        break\n",
    "#     test_loop(test_dataloader, PSP_Rnet, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e549dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
