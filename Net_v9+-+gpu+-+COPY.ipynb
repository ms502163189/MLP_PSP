{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator \n",
    "# - using the synthetic images to train our network\n",
    "import os\n",
    "import numpy as np\n",
    "from config import config\n",
    "from fringes import fringe_wrapper\n",
    "from fringes import Fringes_Generator \n",
    "import random\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "import torch,gc\n",
    "\n",
    "cfg = config()\n",
    "cfg.net_dir=\"./data/train_gamma/\"\n",
    "cfg.pattern_size = [1920, 50] #[1920, 50] [512, 1]\n",
    "cfg.Tp = [30, 33, 36]       #在这个实验中，就先设置成这个周期。\n",
    "cfg.steps = [7,4,3]\n",
    "cfg.gamma = 1.1\n",
    "\n",
    "#删除文件\n",
    "def del_files(path_file):\n",
    "    ls = os.listdir(path_file)\n",
    "    for i in ls:\n",
    "        f_path = os.path.join(path_file, i)\n",
    "        # 判断是否是一个目录,若是,则递归删除\n",
    "        if os.path.isdir(f_path):\n",
    "            del_files(f_path)\n",
    "        else:\n",
    "            os.remove(f_path)\n",
    "\n",
    "a = input(\"是否需要重新生成数据集，请输入Y或者N ：\")\n",
    "if a == 'Y':\n",
    "    cfg.pattern_path = './data/fringe_gamma'\n",
    "\n",
    "    if not os.path.exists(cfg.pattern_path):\n",
    "        os.makedirs(cfg.pattern_path)\n",
    "    if not os.path.exists(cfg.net_dir):\n",
    "        os.makedirs(cfg.net_dir)\n",
    "\n",
    "    del_files(cfg.pattern_path+\"/\")\n",
    "    del_files(cfg.net_dir)\n",
    "    for i in range(100):\n",
    "        images = fringe_wrapper(cfg, \"gamma\").generate_all()\n",
    "        np.savez(f\"./data/train_gamma/train_data{i:03d}\", image1=images[0], image2=images[1], image3=images[2]) \n",
    "\n",
    "    # \n",
    "\n",
    "    # # #manner 4\n",
    "    # for ind in range(100):\n",
    "    #     fringe_generator=Fringes_Generator(cfg)\n",
    "    #     fringe_generator.save_data(ind)\n",
    "\n",
    "\n",
    "\n",
    "b = input(\"是否需要重新开始训练，请输入Y或者N ：\")\n",
    "if b == 'Y':\n",
    "    if not os.path.exists(\"./data/model_gamma\"):\n",
    "        os.makedirs(\"./data/model_gamma\")\n",
    "    del_files(\"./data/model_gamma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.C :  tensor([[ 0.8019,  0.3569,  0.3569,  0.8019],\n",
      "        [ 1.0000,  1.4450, -0.8019, -0.8019],\n",
      "        [-0.8019, -0.8019,  1.4450,  1.0000]], device='cuda:0')\n",
      "无保存模型，将从头开始训练！\n"
     ]
    }
   ],
   "source": [
    "# build a MLP model for residual estimation \n",
    "#- The linear part is also considered in this network\n",
    "from torch import nn\n",
    "\n",
    "# 定义训练的设备\n",
    "device = torch.device(\"cuda\") #使用gpu进行训练\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()#清楚cuda缓存\n",
    "\n",
    "class NeuralNetwork(nn.Module): #构建CNN神经网络\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Conv2d(9,  16, 1, stride=1),  #四个卷积层\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 1, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 5, 1, stride=1),\n",
    "            nn.BatchNorm2d(5)\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.resnet =  nn.Sequential(\n",
    "            nn.Conv2d(9, 5, 1, stride=1),\n",
    "            nn.BatchNorm2d(5)\n",
    "            # nn.ReLU()   \n",
    "        )\n",
    "        \n",
    "        A = [[1, np.cos(0), np.sin(0)],                \n",
    "             [1, np.cos(6*np.pi/7), np.sin(6*np.pi/7)],\n",
    "             [1, np.cos(8*np.pi/7), np.sin(8*np.pi/7)]]\n",
    "        B = [[1, np.cos(2*np.pi/7), np.sin(2*np.pi/7)],\n",
    "             [1, np.cos(4*np.pi/7), np.sin(4*np.pi/7)],\n",
    "             [1, np.cos(10*np.pi/7), np.sin(10*np.pi/7)],\n",
    "             [1, np.cos(12*np.pi/7), np.sin(12*np.pi/7)]]\n",
    "        A, B = np.array(A), np.array(B) #列表转化成数组\n",
    "        self.C = torch.Tensor(np.matmul(B, np.linalg.inv(A)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.C = self.C.to(device)\n",
    "        print(\"self.C : \",self.C)\n",
    "        D = [[1, np.cos(0), np.sin(0)],\n",
    "            [1, np.cos(2*np.pi/4), np.sin(2*np.pi/4)],\n",
    "            [1, np.cos(4*np.pi/4), np.sin(4*np.pi/4)]]\n",
    "        E = [[1, np.cos(6*np.pi/4), np.sin(6*np.pi/4)]]\n",
    "        self.F = torch.Tensor(np.matmul(E, np.linalg.pinv(D)).transpose()).float()#B乘以A的转置矩阵就是I4到I7的前面的系数\n",
    "        self.F = self.F.to(device)\n",
    "        self.model_save_dir = \"./data/model_gamma\" #定义保存模型的文件夹\n",
    "        self.p = os.path.join(self.model_save_dir,\"best_model\")\n",
    "        self.last_loss = 9999999999\n",
    "        self.last_epoch = 0\n",
    "        \n",
    "        #对数组A求逆矩阵，与B矩阵相乘\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # linear generator for the 4 images\n",
    "        images = images.to(device)\n",
    "        i7s = images.permute(0, 2, 3,1)            #将tensor的维度换位\n",
    "        # i7s = i7s.to(device)\n",
    "        i7s,_ = torch.split(i7s,(3,6),dim=-1)      #切分，得到的就是I1 I7 I8\n",
    "        mids,another = torch.split(_,(3,3),dim = -1) #切分，得到4步相移的前三步\n",
    "\n",
    "        linearPart = torch.matmul(i7s, self.C+0.0) #Y矩阵乘法 50*(1950*3)矩阵乘以3*4矩阵\n",
    "        linearPart1 = torch.matmul(mids, self.F+0.0)   #矩阵乘法 50*(1950*3)矩阵乘以3*1矩阵\n",
    "        linearPart = linearPart.to(device)\n",
    "        linearPart1 = linearPart1.to(device)\n",
    "        linearPart = linearPart.permute(0,3,1,2)   #维度换位\n",
    "        linearPart1 = linearPart1.permute(0,3,1,2)   #维度换位\n",
    "        linearPart2 = torch.cat([linearPart, linearPart1], 1)\n",
    "        \n",
    "        # The nonliear part is estimated with MLP function \n",
    "        images = images/255\n",
    "        images = images.to(device)\n",
    "        res1 = self.MLP(images)\n",
    "        out1 = res1 + self.resnet(images)\n",
    "        # out = self.resnet1(out1)\n",
    "        # out = self.conv2(out1)\n",
    "        # res2 = self.conv2(out1)\n",
    "        # out = res2 + self.resnet2(out1)\n",
    "        # print_out = nn.ReLU(out)\n",
    "        # print(\"res.shape:{}\".format(res.shape))\n",
    "        # RES = res.view(-1,64*cfg.pattern_size[1]*cfg.pattern_size[0])\n",
    "        # print(\"RES.shape:{}\".format(RES.shape))\n",
    "        # RES = self.classifier(RES)\n",
    "        # RES1 = RES.view(len(RES),-1,cfg.pattern_size[0])\n",
    "        # print(\"RES1.shape:{}\".format(RES1.shape))\n",
    "        # RES2 = RES1.view(len(RES),-1,cfg.pattern_size[1],cfg.pattern_size[0])\n",
    "        # print(\"RES2.shape:{}\".format(RES2.shape))\n",
    "        \n",
    "#         print(\"zhixingle\")\n",
    "        return linearPart2 + out1\n",
    "    \n",
    "    def save(self,optimizer,epoch,loss,batch): #TODO\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        m = os.path.join(self.model_save_dir,\"model\"+str(epoch))\n",
    "\n",
    "        state = {'model':self.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':epoch}\n",
    "        # 保存模型\n",
    "#         if epoch % 100 == 0:\n",
    "#             torch.save(state, m)\n",
    "#             print(\"模型{}保存成功\".format(str(epoch)))\n",
    "        \n",
    "        if (epoch == 0 or self.last_loss == 10000):\n",
    "            self.last_loss = loss\n",
    "            self.last_epoch = epoch\n",
    "            torch.save(state, self.p)\n",
    "            print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "        else:\n",
    "            if (loss <= self.last_loss): #用loss判断最优模型\n",
    "                self.last_loss = loss\n",
    "                self.last_epoch = epoch\n",
    "                torch.save(state, self.p)\n",
    "                print(\"lat_loss {},best_model保存成功,它是epoch{},loss为{},batch为{}\".format(self.last_loss,epoch,loss,batch))\n",
    "            else:\n",
    "                print(\"best_model未保存，最优模型仍为epoch{}\".format(self.last_epoch,self.last_loss))\n",
    "        \n",
    "    def load(self,optimizer): #TODO\n",
    "\n",
    "        # 如果有保存的模型，则加载模型，并在其基础上继续训练\n",
    "        if os.path.exists(self.p):\n",
    "            checkpoint = torch.load(self.p)\n",
    "            self.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            print('加载best_model成功！其为epoch{}'.format(start_epoch))\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            print('无保存模型，将从头开始训练！')\n",
    "        return start_epoch\n",
    "\n",
    "    \n",
    "PSP_Rnet = NeuralNetwork()\n",
    "PSP_Rnet = PSP_Rnet.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.MSELoss() #均方误差\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(PSP_Rnet.parameters(), lr=learning_rate)\n",
    "\n",
    "start_epoch = PSP_Rnet.load(optimizer)\n",
    "\n",
    "# Test the network\n",
    "# TestTensor=torch.Tensor(1,9,1024,512) #数值从0到1，分为9块，每一块中1024行，512列\n",
    "# result = PSP_Rnet(TestTensor)\n",
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class CustomImageDataset(Dataset):#自定义图像数据集\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_name =f\"train_data{idx:03d}.npz\"\n",
    "        data_path = os.path.join(self.img_dir, data_name)\n",
    "        data = np.load(data_path)\n",
    "        img1_0, img1_3, img1_4 = data[\"image1\"][0], data[\"image1\"][3], data[\"image1\"][4]\n",
    "        img2_0, img2_1, img2_2 = data[\"image2\"][0], data[\"image2\"][1], data[\"image2\"][2]\n",
    "        img3_0, img3_1, img3_2 = data[\"image3\"][0], data[\"image3\"][1], data[\"image3\"][2]\n",
    "        img1_1, img1_2, img1_5, img1_6, img2_3 = data[\"image1\"][1], data[\"image1\"][2], data[\"image1\"][5], data[\"image1\"][6], data[\"image2\"][3]\n",
    "        \n",
    "        inputs = np.stack([img1_0, img1_3, img1_4, img2_0, img2_1, img2_2, img3_0, img3_1, img3_2]).astype(np.float32)\n",
    "        outputs = np.stack([img1_1, img1_2, img1_5, img1_6, img2_3]).astype(np.float32)\n",
    "#         print(inputs.shape)\n",
    "#         print(outputs.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return inputs, outputs\n",
    "    \n",
    "training_data = CustomImageDataset(cfg.net_dir)\n",
    "inputs, outputs = training_data[0]\n",
    "# print(inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=16)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4fe8120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 17.569458  [    0/  100]\n",
      "lat_loss 17.5694580078125,best_model保存成功,它是epoch0,loss为17.5694580078125,batch为0\n",
      "batch :  1\n",
      "loss: 17.151508  [   16/  100]\n",
      "lat_loss 17.151508331298828,best_model保存成功,它是epoch0,loss为17.151508331298828,batch为1\n",
      "batch :  2\n",
      "loss: 16.754805  [   32/  100]\n",
      "lat_loss 16.754804611206055,best_model保存成功,它是epoch0,loss为16.754804611206055,batch为2\n",
      "batch :  3\n",
      "loss: 16.383083  [   48/  100]\n",
      "lat_loss 16.38308334350586,best_model保存成功,它是epoch0,loss为16.38308334350586,batch为3\n",
      "batch :  4\n",
      "loss: 16.038660  [   64/  100]\n",
      "lat_loss 16.038660049438477,best_model保存成功,它是epoch0,loss为16.038660049438477,batch为4\n",
      "batch :  5\n",
      "loss: 15.720409  [   80/  100]\n",
      "lat_loss 15.720409393310547,best_model保存成功,它是epoch0,loss为15.720409393310547,batch为5\n",
      "batch :  6\n",
      "loss: 15.426774  [   24/  100]\n",
      "lat_loss 15.426774024963379,best_model保存成功,它是epoch0,loss为15.426774024963379,batch为6\n",
      "loss_list : \n",
      "[17.5694580078125, 17.151508331298828, 16.754804611206055, 16.38308334350586, 16.038660049438477, 15.720409393310547, 15.426774024963379]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 15.154386  [    0/  100]\n",
      "lat_loss 15.154385566711426,best_model保存成功,它是epoch1,loss为15.154385566711426,batch为0\n",
      "batch :  1\n",
      "loss: 14.901462  [   16/  100]\n",
      "lat_loss 14.901461601257324,best_model保存成功,它是epoch1,loss为14.901461601257324,batch为1\n",
      "batch :  2\n",
      "loss: 14.666655  [   32/  100]\n",
      "lat_loss 14.666654586791992,best_model保存成功,它是epoch1,loss为14.666654586791992,batch为2\n",
      "batch :  3\n",
      "loss: 14.448322  [   48/  100]\n",
      "lat_loss 14.448322296142578,best_model保存成功,它是epoch1,loss为14.448322296142578,batch为3\n",
      "batch :  4\n",
      "loss: 14.245024  [   64/  100]\n",
      "lat_loss 14.245023727416992,best_model保存成功,它是epoch1,loss为14.245023727416992,batch为4\n",
      "batch :  5\n",
      "loss: 14.055834  [   80/  100]\n",
      "lat_loss 14.05583381652832,best_model保存成功,它是epoch1,loss为14.05583381652832,batch为5\n",
      "batch :  6\n",
      "loss: 13.879121  [   24/  100]\n",
      "lat_loss 13.879120826721191,best_model保存成功,它是epoch1,loss为13.879120826721191,batch为6\n",
      "loss_list : \n",
      "[15.154385566711426, 14.901461601257324, 14.666654586791992, 14.448322296142578, 14.245023727416992, 14.05583381652832, 13.879120826721191]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 13.712937  [    0/  100]\n",
      "lat_loss 13.712937355041504,best_model保存成功,它是epoch2,loss为13.712937355041504,batch为0\n",
      "batch :  1\n",
      "loss: 13.554914  [   16/  100]\n",
      "lat_loss 13.554913520812988,best_model保存成功,它是epoch2,loss为13.554913520812988,batch为1\n",
      "batch :  2\n",
      "loss: 13.402571  [   32/  100]\n",
      "lat_loss 13.402570724487305,best_model保存成功,它是epoch2,loss为13.402570724487305,batch为2\n",
      "batch :  3\n",
      "loss: 13.254586  [   48/  100]\n",
      "lat_loss 13.254586219787598,best_model保存成功,它是epoch2,loss为13.254586219787598,batch为3\n",
      "batch :  4\n",
      "loss: 13.109129  [   64/  100]\n",
      "lat_loss 13.109128952026367,best_model保存成功,它是epoch2,loss为13.109128952026367,batch为4\n",
      "batch :  5\n",
      "loss: 12.965433  [   80/  100]\n",
      "lat_loss 12.965433120727539,best_model保存成功,它是epoch2,loss为12.965433120727539,batch为5\n",
      "batch :  6\n",
      "loss: 12.822758  [   24/  100]\n",
      "lat_loss 12.822757720947266,best_model保存成功,它是epoch2,loss为12.822757720947266,batch为6\n",
      "loss_list : \n",
      "[13.712937355041504, 13.554913520812988, 13.402570724487305, 13.254586219787598, 13.109128952026367, 12.965433120727539, 12.822757720947266]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 12.681158  [    0/  100]\n",
      "lat_loss 12.681158065795898,best_model保存成功,它是epoch3,loss为12.681158065795898,batch为0\n",
      "batch :  1\n",
      "loss: 12.540798  [   16/  100]\n",
      "lat_loss 12.54079818725586,best_model保存成功,它是epoch3,loss为12.54079818725586,batch为1\n",
      "batch :  2\n",
      "loss: 12.404145  [   32/  100]\n",
      "lat_loss 12.404145240783691,best_model保存成功,它是epoch3,loss为12.404145240783691,batch为2\n",
      "batch :  3\n",
      "loss: 12.272665  [   48/  100]\n",
      "lat_loss 12.272665023803711,best_model保存成功,它是epoch3,loss为12.272665023803711,batch为3\n",
      "batch :  4\n",
      "loss: 12.147025  [   64/  100]\n",
      "lat_loss 12.147025108337402,best_model保存成功,它是epoch3,loss为12.147025108337402,batch为4\n",
      "batch :  5\n",
      "loss: 12.026669  [   80/  100]\n",
      "lat_loss 12.026668548583984,best_model保存成功,它是epoch3,loss为12.026668548583984,batch为5\n",
      "batch :  6\n",
      "loss: 11.912088  [   24/  100]\n",
      "lat_loss 11.912088394165039,best_model保存成功,它是epoch3,loss为11.912088394165039,batch为6\n",
      "loss_list : \n",
      "[12.681158065795898, 12.54079818725586, 12.404145240783691, 12.272665023803711, 12.147025108337402, 12.026668548583984, 11.912088394165039]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 11.802446  [    0/  100]\n",
      "lat_loss 11.802446365356445,best_model保存成功,它是epoch4,loss为11.802446365356445,batch为0\n",
      "batch :  1\n",
      "loss: 11.696691  [   16/  100]\n",
      "lat_loss 11.696690559387207,best_model保存成功,它是epoch4,loss为11.696690559387207,batch为1\n",
      "batch :  2\n",
      "loss: 11.594265  [   32/  100]\n",
      "lat_loss 11.59426498413086,best_model保存成功,它是epoch4,loss为11.59426498413086,batch为2\n",
      "batch :  3\n",
      "loss: 11.495384  [   48/  100]\n",
      "lat_loss 11.495384216308594,best_model保存成功,它是epoch4,loss为11.495384216308594,batch为3\n",
      "batch :  4\n",
      "loss: 11.400154  [   64/  100]\n",
      "lat_loss 11.400154113769531,best_model保存成功,它是epoch4,loss为11.400154113769531,batch为4\n",
      "batch :  5\n",
      "loss: 11.309440  [   80/  100]\n",
      "lat_loss 11.309439659118652,best_model保存成功,它是epoch4,loss为11.309439659118652,batch为5\n",
      "batch :  6\n",
      "loss: 11.223457  [   24/  100]\n",
      "lat_loss 11.223457336425781,best_model保存成功,它是epoch4,loss为11.223457336425781,batch为6\n",
      "loss_list : \n",
      "[11.802446365356445, 11.696690559387207, 11.59426498413086, 11.495384216308594, 11.400154113769531, 11.309439659118652, 11.223457336425781]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "size :  100\n",
      "batch :  0\n",
      "loss: 11.142179  [    0/  100]\n",
      "lat_loss 11.142178535461426,best_model保存成功,它是epoch5,loss为11.142178535461426,batch为0\n",
      "batch :  1\n",
      "loss: 11.064669  [   16/  100]\n",
      "lat_loss 11.064668655395508,best_model保存成功,它是epoch5,loss为11.064668655395508,batch为1\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer,loss_list):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"size : \",size)\n",
    "    for data in enumerate(dataloader):#batch, (X, y)\n",
    "        batch, imgs = data\n",
    "        # batch = batch.to(device)\n",
    "        # imgs = imgs.to(device)\n",
    "        # Compute prediction and loss\n",
    "#         print(\"X.shape : \",X.shape)\n",
    "# #         print(y)\n",
    "# #         print(X)\n",
    "#         print(\"y.shape : \",y.shape)\n",
    "        X,y = imgs\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        print(\"batch : \",batch)\n",
    "        model = model.to(device)\n",
    "        pred = model(X)          # 前向传播计算预测值\n",
    "        pred_type = pred.is_cuda\n",
    "        pred = pred.to(device)\n",
    "        loss_fn = loss_fn.to(device)\n",
    "        loss = loss_fn(pred, y)  #真实值与预测值求均方误差\n",
    "#         print(pred-y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # 将模型的参数梯度初始化为0\n",
    "        loss = loss.to(device)\n",
    "        loss.backward()         # 反向传播计算梯度\n",
    "        optimizer.step()        # 更新所有参数\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             model.save()\n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        if len(loss_list) < 100//16 + 1:\n",
    "            loss_list.append(loss)\n",
    "        else:    \n",
    "            loss_list[batch] = loss\n",
    "            \n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        PSP_Rnet.save(optimizer,t,loss,batch)\n",
    "    return loss\n",
    "        \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "if not os.path.exists('./path/to/log'):\n",
    "    os.makedirs('./path/to/log')\n",
    "writer = SummaryWriter('./path/to/log')\n",
    "epochs = 200000\n",
    "loss_list = []\n",
    "for t in range(start_epoch,epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    loss = train_loop(train_dataloader, PSP_Rnet, loss_fn, optimizer,loss_list)\n",
    "    # writer.add_scalar('manner15_gpu_loss',loss, t)\n",
    "    writer.add_scalars('manner2_gamma_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6]}, t)\n",
    "    # writer.add_scalars('manner_gpu_loss',{'batch0':loss_list[0],'batch1':loss_list[1],'batch2':loss_list[2],'batch3':loss_list[3],'batch4':loss_list[4],'batch5':loss_list[5],'batch6':loss_list[6],'batch7':loss_list[7],'batch8':loss_list[8],'batch9':loss_list[9],'batch10':loss_list[10],'batch11':loss_list[11],'batch12':loss_list[12],'batch13':loss_list[13],'batch14':loss_list[14],'batch15':loss_list[15]}, t)\n",
    "    print('loss_list : ')\n",
    "    print(loss_list)\n",
    "\n",
    "    sum = 0\n",
    "    element = 0\n",
    "    for i in loss_list:\n",
    "        element += 1\n",
    "        if i <= 0.2:\n",
    "            sum += 1\n",
    "    if sum == element and element > 0:\n",
    "        break\n",
    "#     test_loop(test_dataloader, PSP_Rnet, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e549dff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
